{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbbJdvsL6eBfDJDP8tYlZc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alyson-mei/ml_math_hw_1/blob/main/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm4Ws4FQyrfK",
        "outputId": "ae8ee3aa-9bd6-43c5-e7c2-13b57b93448a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ru-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.7.0/ru_core_news_sm-3.7.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from ru-core-news-sm==3.7.0) (3.7.4)\n",
            "Collecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.7.0)\n",
            "  Downloading pymorphy3-2.0.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m996.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download ru_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path_do_data = '../../datasets/Machine_translation_EN_RU/data.txt'\n",
        "if not os.path.exists(path_do_data):\n",
        "    print(\"Dataset not found locally. Downloading from github.\")\n",
        "    !wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/datasets/Machine_translation_EN_RU/data.txt -nc\n",
        "    path_do_data = './data.txt'"
      ],
      "metadata": {
        "id": "SR7_GJEpzWfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchdata.datapipes as dp\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchtext\n",
        "import torchtext.transforms as T\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import spacy\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "metadata": {
        "id": "YDa_3v9NzGBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "a7t8X_SL4heA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "v6IaKp2r4lQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en = spacy.load(\"en_core_web_sm\")\n",
        "ru = spacy.load(\"ru_core_news_sm\")"
      ],
      "metadata": {
        "id": "7vCybaIJzsiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_pipe = dp.iter.IterableWrapper([path_do_data]) #creating an iterable of filenames\n",
        "data_pipe = dp.iter.FileOpener(data_pipe, mode='rb') #pass the iterable to FileOpener which then opens the file in read mode\n",
        "data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True) #call a function to parse the file"
      ],
      "metadata": {
        "id": "O5GQW9Qm1Q3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "%%script false --no-raise-error\n",
        "\n",
        "dp_list = list(data_pipe)\n",
        "total_len = len(dp_list)\n",
        "dp_list[:5]"
      ],
      "metadata": {
        "id": "rvqIFnue1Yw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enTokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenize an English text and return a list of tokens\n",
        "    \"\"\"\n",
        "    return [token.text for token in en.tokenizer(text)]\n",
        "\n",
        "def ruTokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenize a Russian text and return a list of tokens\n",
        "    \"\"\"\n",
        "    return [token.text for token in ru.tokenizer(text)]"
      ],
      "metadata": {
        "id": "cB25_vsC1zvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "%%script false --no-raise-error\n",
        "\n",
        "print(enTokenize(dp_list[0][0]))\n",
        "print(ruTokenize(dp_list[0][1]))"
      ],
      "metadata": {
        "id": "sVrGTbrx3saa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the vocabulary\n"
      ],
      "metadata": {
        "id": "Rl-ncvy04nWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getTokens(data_iter, place):\n",
        "    \"\"\"\n",
        "    Function to yield tokens from an iterator. Since, our iterator contains\n",
        "    tuple of sentences (source and target), `place` parameters defines for which\n",
        "    index to return the tokens for. `place = 0` for source and `place = 1` for target\n",
        "    \"\"\"\n",
        "    for english, russian in data_iter:\n",
        "        if place == 0:\n",
        "            yield enTokenize(english)\n",
        "        else:\n",
        "            yield ruTokenize(russian)"
      ],
      "metadata": {
        "id": "15Wp9ImU411R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_vocab = build_vocab_from_iterator(\n",
        "    getTokens(data_pipe, 0),\n",
        "    min_freq = 2,\n",
        "    specials = ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
        "    special_first=True\n",
        ")\n",
        "source_vocab.set_default_index(source_vocab['<unk>'])\n",
        "\n",
        "target_vocab = build_vocab_from_iterator(\n",
        "    getTokens(data_pipe,1),\n",
        "    min_freq = 2,\n",
        "    specials = ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
        "    special_first=True\n",
        ")\n",
        "target_vocab.set_default_index(target_vocab['<unk>'])"
      ],
      "metadata": {
        "id": "VNLVa2Rv5M58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "%%script false --no-raise-error\n",
        "\n",
        "print(source_vocab.get_itos()[:15])\n",
        "print(target_vocab.get_itos()[:15])\n",
        "print(\"Matarese\" in source_vocab)"
      ],
      "metadata": {
        "id": "Bk4IaYnz6-gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numericalize sentences using vocabulary\n"
      ],
      "metadata": {
        "id": "jSp-flNh7Na4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getTransform(vocab):\n",
        "    \"\"\"\n",
        "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
        "    of tokens.\n",
        "    \"\"\"\n",
        "    text_tranform = T.Sequential(\n",
        "        ## converts the sentences to indices based on given vocabulary\n",
        "        T.VocabTransform(vocab = vocab),\n",
        "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
        "        # 1 as seen in previous section\n",
        "        T.AddToken(1, begin = True),\n",
        "        ## Add <eos> at ending of each sentence. 2 because the index for <eos> in vocabulary is\n",
        "        # 2 as seen in previous section\n",
        "        T.AddToken(2, begin = False)\n",
        "    )\n",
        "    return text_tranform"
      ],
      "metadata": {
        "id": "gI-oz-Gq7Lhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "%%script false --no-raise-error\n",
        "\n",
        "some_sentence = list(data_pipe)[798][0]\n",
        "print(\"Some sentence = \", end = \"\")\n",
        "print(some_sentence)\n",
        "transformed_sentence = getTransform(source_vocab)(enTokenize(some_sentence))\n",
        "print(\"Transformed sentence = \", end = \"\")\n",
        "print(transformed_sentence)\n",
        "index_to_string = source_vocab.get_itos()\n",
        "for index in transformed_sentence:\n",
        "    print(index_to_string[index], end = \" \")"
      ],
      "metadata": {
        "id": "r6MCGXGW7k3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def applyTransform(sequence_pair):\n",
        "    \"\"\"\n",
        "    Apply transforms to sequence of tokens in a sequence pair\n",
        "    \"\"\"\n",
        "    return (\n",
        "        getTransform(source_vocab)(enTokenize(sequence_pair[0])),\n",
        "        getTransform(target_vocab)(ruTokenize(sequence_pair[1]))\n",
        "    )\n",
        "\n",
        "data_pipe = data_pipe.map(applyTransform) ## Apply the function to each element in the iterator\n"
      ],
      "metadata": {
        "id": "9fiI8-jb-y7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "%%script false --no-raise-error\n",
        "\n",
        "for sample in data_pipe:\n",
        "    print(sample)\n",
        "    break"
      ],
      "metadata": {
        "id": "0pAliodW_vVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make batches (bucket_batch)\n"
      ],
      "metadata": {
        "id": "eqgYIpJdAnro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sortBucket(bucket):\n",
        "    \"\"\"\n",
        "    Function to sort a given bucket. Here, we want to sort based on the length of\n",
        "    source and target sequence.\n",
        "    \"\"\"\n",
        "    return sorted(bucket, key=lambda x: (len(x[0]), len(x[1])))\n",
        "\n",
        "data_pipe = data_pipe.bucketbatch(\n",
        "    batch_size = 64,\n",
        "    bucket_num = 1,\n",
        "    use_in_batch_shuffle = True,\n",
        "    sort_key = sortBucket\n",
        ")"
      ],
      "metadata": {
        "id": "8Mi2J4U9Cfwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "%%script false --no-raise-error\n",
        "\n",
        "for sample in data_pipe:\n",
        "    print(sample[:4])\n",
        "    break\n",
        "print(len(list(data_pipe)))"
      ],
      "metadata": {
        "id": "g8Paie9GD_8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def separateSourceTarget(sequence_pairs):\n",
        "    \"\"\"\n",
        "    input of form: `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`\n",
        "    output of form: `((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))`\n",
        "    \"\"\"\n",
        "    sources, targets = zip(*sequence_pairs)\n",
        "    return sources, targets\n",
        "\n",
        "## Apply the function to each element in the iterator\n",
        "data_pipe = data_pipe.map(separateSourceTarget)"
      ],
      "metadata": {
        "id": "8Zpf5uoXErp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "%%script false --no-raise-error\n",
        "\n",
        "for sample in data_pipe:\n",
        "    print(len(sample))\n",
        "    print(sample[0])\n",
        "    print(sample[1])\n",
        "    break"
      ],
      "metadata": {
        "id": "3IrzSTJxFRYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding"
      ],
      "metadata": {
        "id": "Aohyf5ZIGU6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def applyPadding(pair_of_sequences):\n",
        "    \"\"\"\n",
        "    Convert sequences to tensors and apply padding\n",
        "    \"\"\"\n",
        "    return (T.ToTensor(0)(list(pair_of_sequences[0])), T.ToTensor(0)(list(pair_of_sequences[1])))\n",
        "## `T.ToTensor(0)` returns a transform that converts the sequence to `torch.tensor` and also applies\n",
        "# padding. Here, `0` is passed to the constructor to specify the index of the `<pad>` token in the\n",
        "# vocabulary.\n",
        "data_pipe = data_pipe.map(applyPadding)"
      ],
      "metadata": {
        "id": "oqWRsh7GGYWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "%%script false --no-raise-error\n",
        "\n",
        "for sample in data_pipe:\n",
        "    print(len(sample))\n",
        "    print(sample[0].shape)\n",
        "    print(sample[0])\n",
        "    print(sample[1].shape)\n",
        "    print(sample[1])\n",
        "    break"
      ],
      "metadata": {
        "id": "wIZWfRVWGa4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, valid, test = data_pipe.random_split(total_length = total_len,\n",
        "                                      weights={\"train\": 0.8, \"valid\": 0.1, \"test\": 0.1},\n",
        "                                      seed = 0)"
      ],
      "metadata": {
        "id": "1ER25KCdOY4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "%%script false --no-raise-error\n",
        "\n",
        "for sample in train:\n",
        "    print(sample)\n",
        "    break"
      ],
      "metadata": {
        "id": "5WqoXESiOoSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    dataset = train,\n",
        "    num_workers = 2)\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    dataset = valid,\n",
        "    num_workers = 2)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset = test,\n",
        "    num_workers = 2)"
      ],
      "metadata": {
        "id": "fNghap3cK0KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(train_loader):\n",
        "    print(batch[0])\n",
        "    break"
      ],
      "metadata": {
        "id": "iD8wSoz4vGV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq model"
      ],
      "metadata": {
        "id": "o_yuoixyk2nC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "xSuJIenS6jKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "    def forward(self, source):\n",
        "        # source: (seq_length, batch_size)\n",
        "        print(1)\n",
        "\n",
        "        embedding = self.dropout(self.embedding(source))\n",
        "        # embedding: (seq_length, batch_size, embedding_size)\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedding)\n",
        "        # hidden: (num_layers, batch_size, hidden_size)\n",
        "        # cell: (n_layers, batch_size, hidden_size)\n",
        "\n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "vr8U1iuok3cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layesrs = num_layers\n",
        "        self.output_size = output_size\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "def forward(self, input, hidden, cell):\n",
        "    # input: batch_size\n",
        "    # hidden: (n_layers, batch_size, hidden_size)\n",
        "    # cell: (n_layers, batch_size, hidden_size)\n",
        "\n",
        "    # input: batch_size, but we want (1, batch_size)\n",
        "    input = input.unsqueeze(0)\n",
        "\n",
        "    embedding = self.dropout(self.embedding(input))\n",
        "    # embedding: (seq_length, batch_size, embedding_size)\n",
        "\n",
        "    output, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "    # output: (1, batch_size, hidden_size)\n",
        "\n",
        "    # shape of predictions: (1, N, length_of_vocab)\n",
        "    predictions = self.fc(output.squeeze(0))\n",
        "\n",
        "    # prediction = (batch size, output dim)\n",
        "\n",
        "    return predictions, hidden, cell"
      ],
      "metadata": {
        "id": "vn1KlCq9oZSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ration = 0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = self.decoder.output_size\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        # Grab start token\n",
        "        input = target[0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            outputs[t] = output\n",
        "\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            input = target[t] if random.random() < teacher_force_ration else best_guess\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "0Wt0S9b1tGxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "tN4TdZf36fhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "num_epochs = 20\n",
        "learning_rate = 1.e-3\n",
        "\n",
        "# Model hyperparameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size_encoder = len(source_vocab)\n",
        "input_size_decoder = len(target_vocab)\n",
        "output_size = len(target_vocab)\n",
        "encoder_embedding_size = 256\n",
        "decoder_embedding_size = 256\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5\n",
        "\n",
        "# Tensorboard\n",
        "writer = SummaryWriter(f'runs/Loss_plot')\n",
        "step = 0"
      ],
      "metadata": {
        "id": "mtipMcH76khd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_net = Encoder(input_size_encoder,\n",
        "                      encoder_embedding_size,\n",
        "                      hidden_size,\n",
        "                      num_layers,\n",
        "                      dec_dropout).to(device)\n",
        "\n",
        "decoder_net = Decoder(input_size_encoder,\n",
        "                      encoder_embedding_size,\n",
        "                      hidden_size,\n",
        "                      output_size,\n",
        "                      num_layers,\n",
        "                      dec_dropout).to(device)\n",
        "\n",
        "model = Seq2Seq(encoder_net, decoder_net).to(device)"
      ],
      "metadata": {
        "id": "f-6W3HNrpkMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "id": "EXX7ayZ0tZa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_idx = source_vocab.__getitem__(\"<pad>\")\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "BuXkAez7rEqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(train_loader):\n",
        "    input = batch[0].to(device)\n",
        "    target = batch[1].to(device)\n",
        "\n",
        "    output = model(input, target)\n",
        "    #output: (trg_len, batch_size, output_dim)\n",
        "\n",
        "    output_dim = output.shape[-1]\n",
        "    output = output[1:].reshape(-1, output.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1)\n",
        "    optimizer.step()\n",
        "\n",
        "    writer.add_scalar('Training Loss', loss, global_step = step)\n",
        "    step += 1"
      ],
      "metadata": {
        "id": "Y3zKdwf9sIe4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}